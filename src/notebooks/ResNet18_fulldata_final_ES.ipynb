{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Clasificación de Radiografías Musculoesqueléticas (MURA)**\n",
    "\n",
    "Este proyecto tiene como objetivo desarrollar un modelo de **clasificación binaria** capaz de identificar si una radiografía musculoesquelética (perteneciente al conjunto de datos [MURA](https://stanfordmlgroup.github.io/competitions/mura/)) presenta una **anomalía (abnormal)** o es **normal (negative)**.\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Qué haremos?\n",
    "\n",
    "1. **Prepararemos los datos** a partir de las imágenes del dataset MURA.\n",
    "2. **Crearemos un modelo basado en ResNet18 preentrenado**, adaptado para clasificación binaria.\n",
    "3. **Entrenaremos** el modelo sobre las radiografías con técnicas de fine-tuning.\n",
    "4. **Evaluaremos su rendimiento** con métricas como F1-score, accuracy y matriz de confusión.\n",
    "5. **Visualizaremos predicciones correctas y errores** para interpretar el comportamiento.\n",
    "6. **Guardaremos el modelo final** para uso futuro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Paso 1: Importaciones y librerías necesarias**\n",
    "\n",
    "A continuación, cargamos todas las librerías necesarias para procesamiento de imágenes, modelado con PyTorch, visualización y evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías de procesamiento y visualización\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# PyTorch y herramientas para modelado\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Guardado de modelo\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Paso 2: Configuración e Hiperparámetros**\n",
    "\n",
    "En este bloque definimos los valores clave para entrenar nuestro modelo:\n",
    "- Ruta del dataset MURA completo (guardado en local debido al peso)\n",
    "- Tamaño de imagen y lote\n",
    "- Número de épocas\n",
    "- Learning rate\n",
    "- Patiencia para early stopping y reducción de LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computando en: mps\n"
     ]
    }
   ],
   "source": [
    "# Ruta al dataset MURA completo\n",
    "DATASET_DIR = \"/Users/alvarosanchez/Downloads/MURA-v1.1\"\n",
    "train_dir = os.path.join(DATASET_DIR, \"train\")\n",
    "valid_dir = os.path.join(DATASET_DIR, \"valid\")\n",
    "\n",
    "# Dispositivo (GPU si está disponible)\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Computando en:\", DEVICE)\n",
    "\n",
    "# Tamaño de imagen\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "# Hiperparámetros\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "EARLYSTOP_PATIENCE = 5\n",
    "LR_PATIENCE = 5\n",
    "WEIGHT_DECAY = 1e-4  # Regularización L2 para evitar overfitting\n",
    "\n",
    "# Ruta donde se guardará el modelo final\n",
    "MODEL_PATH = \"/Users/alvarosanchez/ONLINE_DS_THEBRIDGE_ALVAROSMMS-1/ML_Clasificacion_Radiografias_Muscoesqueleticas/src/models/resnet18_final_model.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Paso 3: Transformaciones, creación del Dataset y DataLoaders**\n",
    "\n",
    "Para entrenar un modelo robusto, es crucial preparar adecuadamente los datos. En este paso:\n",
    "\n",
    "- Aplicaremos **transformaciones de aumento de datos (data augmentation)** al conjunto de entrenamiento para mejorar la capacidad de generalización del modelo.\n",
    "- Estandarizaremos todas las imágenes al mismo tamaño (`224x224`) y normalizaremos con la media y desviación estándar de ImageNet (ya que la ResNet fue preentrenada con este dataset).\n",
    "- Crearemos los `DataLoader` de entrenamiento y validación para facilitar la iteración por lotes durante el entrenamiento.\n",
    "\n",
    "Además, incluimos un pequeño análisis para detectar posibles problemas de estructura de carpetas o archivos ocultos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado: 36808 entrenamiento, 3197 validación\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Transformaciones de imágenes\n",
    "# ---------------------------\n",
    "\n",
    "# Transformaciones para el conjunto de entrenamiento\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),                      # Redimensiona todas las imágenes a 224x224 píxeles\n",
    "    transforms.RandomHorizontalFlip(),                 # Aplica un flip horizontal aleatorio (50% de probabilidad)\n",
    "    transforms.RandomRotation(degrees=10),             # Rota aleatoriamente las imágenes entre -10 y 10 grados\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Ajustes aleatorios de brillo y contraste\n",
    "    transforms.ToTensor(),                             # Convierte la imagen PIL a tensor (necesario para PyTorch)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],    # Normaliza con los valores de la media y desviación\n",
    "                         std=[0.229, 0.224, 0.225])     # usados en los modelos preentrenados de ImageNet\n",
    "])\n",
    "\n",
    "# Transformaciones para el conjunto de validación (sin augmentación)\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Dataset personalizado: MURABinaryDataset\n",
    "# ---------------------------------------------\n",
    "class MURABinaryDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        root_dir = Path(root_dir)\n",
    "\n",
    "        # Recorrido por cada parte del cuerpo (e.g. XR_HAND, XR_WRIST...)\n",
    "        for body_part in root_dir.iterdir():\n",
    "            if not body_part.is_dir() or body_part.name.startswith('.'):\n",
    "                continue  # Saltar archivos ocultos como .DS_Store\n",
    "\n",
    "            for patient_dir in body_part.iterdir():\n",
    "                if not patient_dir.is_dir():\n",
    "                    continue\n",
    "\n",
    "                for study_dir in patient_dir.iterdir():\n",
    "                    if not study_dir.is_dir():\n",
    "                        continue\n",
    "\n",
    "                    # Asignación binaria: 0 (negativa), 1 (positiva)\n",
    "                    label = 1 if \"positive\" in study_dir.name else 0\n",
    "\n",
    "                    # Añadir todas las imágenes del estudio a la lista de muestras\n",
    "                    for img_path in study_dir.glob(\"*.png\"):\n",
    "                        self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Asegurarse de que todas las imágenes tengan 3 canales\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# ---------------------------\n",
    "# Carga de los datasets\n",
    "# ---------------------------\n",
    "\n",
    "# Se crean los datasets personalizados con las transformaciones adecuadas\n",
    "train_dataset = MURABinaryDataset(train_dir, transform=transform_train)\n",
    "valid_dataset = MURABinaryDataset(valid_dir, transform=transform_val)\n",
    "\n",
    "# ---------------------------\n",
    "# Creación de los DataLoaders\n",
    "# ---------------------------\n",
    "\n",
    "# DataLoader para entrenamiento (mezcla aleatoriamente las imágenes)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# DataLoader para validación (no se mezcla, se mantienen en orden)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Verificación del tamaño de los datasets\n",
    "print(f\"Dataset cargado: {len(train_dataset)} entrenamiento, {len(valid_dataset)} validación\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se han detectado **36,808 imágenes de entrenamiento** y **3,197 de validación**, lo cual indica una partición saludable del dataset. Este volumen de datos es suficiente para entrenar modelos de Deep Learning como ResNet18 con un buen rendimiento.\n",
    "\n",
    "Además, se implementa el control de calidad eliminando archivos ocultos (`.DS_Store`) y asegurando que todas las imágenes tengan formato RGB, lo cual es fundamental para trabajar con modelos preentrenados en ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Paso 4: Análisis del desequilibrio de clases**\n",
    "\n",
    "Antes de entrenar el modelo, es crucial analizar la distribución de las clases en el conjunto de entrenamiento.  \n",
    "En este problema, queremos clasificar radiografías como **normales (0)** o **anormales (1)**.  \n",
    "Si hay una diferencia significativa entre el número de imágenes por clase, el modelo podría sesgarse hacia la clase mayoritaria.\n",
    "\n",
    "En este paso:\n",
    "\n",
    "- Calcularemos cuántas imágenes hay de cada clase.\n",
    "- Visualizaremos esa distribución.\n",
    "- Calcularemos **pesos inversamente proporcionales** al número de muestras por clase para equilibrar la función de pérdida (`CrossEntropyLoss`), de forma que el modelo aprenda sin favorecer la clase dominante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases en el conjunto de entrenamiento:\n",
      "Clase 0 (Normal (0)): 21935 ejemplos\n",
      "Clase 1 (Abnormal (1)): 14873 ejemplos\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos calculados para la función de pérdida: [1.6780487804878048, 2.4748201438848922]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Contar la cantidad de ejemplos por clase en el dataset de entrenamiento\n",
    "# Esto permite detectar si hay desbalance entre clases (muy común en problemas médicos)\n",
    "labels = [label for _, label in train_dataset]  # Extrae solo los labels (0 o 1)\n",
    "label_counts = Counter(labels)  # Cuenta cuántas veces aparece cada clase\n",
    "\n",
    "# Imprimir la distribución numérica por clase\n",
    "print(\"Distribución de clases en el conjunto de entrenamiento:\")\n",
    "for clase, cantidad in label_counts.items():\n",
    "    etiqueta = \"Normal (0)\" if clase == 0 else \"Abnormal (1)\"\n",
    "    print(f\"Clase {clase} ({etiqueta}): {cantidad} ejemplos\")\n",
    "\n",
    "# Visualización gráfica de la distribución\n",
    "plt.bar(['Normal (0)', 'Abnormal (1)'], \n",
    "        [label_counts[0], label_counts[1]], \n",
    "        color=['#1f77b4', '#ff7f0e'])\n",
    "plt.title(\"Distribución de clases en el conjunto de entrenamiento\")\n",
    "plt.ylabel(\"Número de imágenes\")\n",
    "plt.show()\n",
    "\n",
    "# Calcular pesos de clase para la función de pérdida\n",
    "# De esta forma, la clase menos representada tendrá un peso mayor\n",
    "total = sum(label_counts.values())\n",
    "class_weights = [total / label_counts[0], total / label_counts[1]]\n",
    "\n",
    "# Convertimos los pesos a tensor y los enviamos al dispositivo adecuado (MPS)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "# Imprimir los pesos calculados, que se usarán en la función CrossEntropyLoss\n",
    "print(f\"\\nPesos calculados para la función de pérdida: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los principales retos al trabajar con datasets médicos como MURA es el **desbalance entre clases**. En este paso, analizamos la distribución de imágenes en el conjunto de entrenamiento y observamos que:\n",
    "\n",
    "- **Clase 0 (Normal)** tiene 21,935 imágenes.\n",
    "- **Clase 1 (Abnormal)** tiene 14,873 imágenes.\n",
    "\n",
    "Esto implica que el modelo podría inclinarse a predecir la clase mayoritaria (Normal), lo cual sería perjudicial para la detección de casos anormales. \n",
    "Para mitigar este problema:\n",
    "\n",
    "- Se calculan **pesos de clase** inversamente proporcionales a la frecuencia de cada clase, resultando en:\n",
    "\n",
    "```python\n",
    "Pesos para la función de pérdida: [1.678, 2.475]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Paso 5: Definición del modelo ResNet18 y descongelado parcial**\n",
    "\n",
    "En este paso, se define el modelo de aprendizaje profundo basado en **ResNet18**, una red convolucional preentrenada en **ImageNet**.\n",
    "\n",
    "Como estrategia de optimización, **descongelamos parcialmente** las capas desde `layer2` en adelante (`layer2`, `layer3`, `layer4` y `fc`).  \n",
    "Esto permite que el modelo ajuste sus pesos en capas más profundas para aprender patrones más específicos del dominio de rayos X musculoesqueléticos, manteniendo congeladas las primeras capas que capturan patrones visuales más generales.\n",
    "\n",
    "Además, se modifica la **capa final (`fc`)** del modelo para realizar **clasificación binaria**, distinguiendo entre radiografías **normales (0)** y **anormales (1)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Cargar la arquitectura ResNet18 preentrenada con pesos de ImageNet\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Descongelamos capas desde 'layer2' en adelante (layer2, layer3, layer4 y fc)\n",
    "# Esto permite que las capas más profundas de la red se reajusten a nuestro problema específico (radiografías de extremidades)\n",
    "# Las capas más bajas (como conv1 y layer1) permanecen congeladas, ya que aprenden características generales muy útiles (bordes, texturas, etc.)\n",
    "for name, param in resnet18.named_parameters():\n",
    "    if name.startswith('layer2') or name.startswith('layer3') or name.startswith('layer4') or name.startswith('fc'):\n",
    "        param.requires_grad = True  # Permitimos que estos pesos se actualicen durante el entrenamiento\n",
    "    else:\n",
    "        param.requires_grad = False  # Mantenemos congelados los pesos del resto (no se actualizan)\n",
    "\n",
    "# Reemplazamos la capa final 'fc' (fully connected)\n",
    "# La original está diseñada para clasificar entre 1000 clases de ImageNet, pero nosotros solo tenemos 2 clases: normal y abnormal\n",
    "num_features = resnet18.fc.in_features  # Número de entradas que espera la capa final\n",
    "resnet18.fc = nn.Linear(num_features, 2)  # Salida con 2 clases: 0 (normal) y 1 (abnormal)\n",
    "\n",
    "# Enviamos el modelo al dispositivo de cómputo (MPS)\n",
    "resnet18 = resnet18.to(DEVICE)\n",
    "\n",
    "# Ver la arquitectura completa del modelo\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Paso 6: Configuración del entrenamiento**\n",
    "\n",
    "En este paso se define todo lo necesario para el entrenamiento del modelo:\n",
    "\n",
    "- La **función de pérdida** (`CrossEntropyLoss`) adaptada al posible desbalance de clases usando pesos.\n",
    "- El **optimizador** (`Adam`) que actualizará los pesos del modelo.\n",
    "- Un **scheduler** (`ReduceLROnPlateau`) que reduce automáticamente la tasa de aprendizaje si el modelo deja de mejorar en validación.\n",
    "- Parámetros de **early stopping** que interrumpen el entrenamiento si el modelo no mejora después de varios *epochs* consecutivos.\n",
    "\n",
    "Esta configuración permite un entrenamiento más estable, eficiente y adaptado a las características del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Calcular pesos de clase para abordar el desbalance de etiquetas\n",
    "class_counts = [label for _, label in train_dataset]  # Extraemos todas las etiquetas del dataset\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(class_counts), y=class_counts)\n",
    "# Resultado: peso mayor para la clase minoritaria (para penalizar más sus errores)\n",
    "\n",
    "# Convertimos los pesos a tensor de PyTorch y lo enviamos al dispositivo\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "# Definición de la función de pérdida (loss function)\n",
    "# Usamos CrossEntropyLoss con pesos personalizados para que el modelo no ignore la clase minoritaria\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Definición del optimizador (Adam en este caso)\n",
    "# Adam adapta automáticamente la tasa de aprendizaje para cada parámetro\n",
    "# weight_decay añade regularización L2 para evitar sobreajuste\n",
    "optimizer = optim.Adam(resnet18.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Configuración del scheduler (ReduceLROnPlateau)\n",
    "# Reduce el learning rate a la mitad si la pérdida de validación no mejora durante cierto número de epochs\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',            # Queremos minimizar la pérdida de validación\n",
    "    factor=0.5,            # Reduce LR multiplicándolo por 0.5\n",
    "    patience=LR_PATIENCE,  # Nº de epochs sin mejora antes de reducir LR\n",
    "    verbose=True           # Muestra mensajes cuando el LR cambia\n",
    ")\n",
    "\n",
    "# Variables para controlar el early stopping manualmente\n",
    "# Si el modelo no mejora tras varios epochs, detenemos el entrenamiento\n",
    "best_val_loss = float('inf')  # Pérdida de validación más baja registrada\n",
    "epochs_no_improve = 0         # Contador de epochs sin mejora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paso 7: Entrenamiento del modelo**\n",
    "\n",
    "En este paso se entrena el modelo ResNet18 con las configuraciones previamente definidas.  \n",
    "Se aplicará Early Stopping para evitar overfitting si la pérdida de validación no mejora durante `EARLYSTOP_PATIENCE` épocas.  \n",
    "También se monitorizan métricas clave como la `accuracy`, el `F1-score`, y la pérdida (`loss`) tanto en entrenamiento como en validación.\n",
    "\n",
    "Además, si se alcanza una mejor puntuación en validación, el modelo se guarda automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comienza el entrenamiento...\n",
      "Epoch 1 | Train Loss: 0.5380 | Val Loss: 0.5063 | Acc: 0.7707 | F1: 0.7402\n",
      "Mejor modelo guardado.\n"
     ]
    }
   ],
   "source": [
    "print(\"Comienza el entrenamiento...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    resnet18.train()  # Ponemos el modelo en modo entrenamiento\n",
    "    train_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    # Bucle sobre los batches del set de entrenamiento\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()  # Reseteamos los gradientes del optimizador\n",
    "        outputs = resnet18(images)  # Hacemos predicciones con el modelo\n",
    "        loss = criterion(outputs, labels)  # Calculamos la pérdida respecto a las etiquetas reales\n",
    "        loss.backward()  # Calculamos los gradientes (backpropagation)\n",
    "        optimizer.step()  # Actualizamos los pesos\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)  # Acumulamos pérdida total del batch\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)  # Obtenemos las clases predichas (mayor probabilidad)\n",
    "        y_true.extend(labels.cpu().numpy())  # Guardamos etiquetas reales\n",
    "        y_pred.extend(preds.cpu().numpy())   # Guardamos predicciones\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)  # Pérdida media del entrenamiento\n",
    "\n",
    "    # Validación (sin actualización de pesos)\n",
    "    resnet18.eval()  # Ponemos el modelo en modo evaluación\n",
    "    val_loss = 0.0\n",
    "    val_true, val_pred = [], []\n",
    "\n",
    "    with torch.no_grad():  # No calculamos gradientes (más rápido y eficiente)\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = resnet18(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "            val_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_loader.dataset)  # Pérdida media de validación\n",
    "    scheduler.step(avg_val_loss)  # Actualizamos el LR si la validación no mejora\n",
    "\n",
    "    # Cálculo de métricas de validación\n",
    "    acc = accuracy_score(val_true, val_pred)  # Exactitud\n",
    "    f1 = f1_score(val_true, val_pred, average=\"binary\")  # F1 para clasificación binaria\n",
    "\n",
    "    # Imprimimos resultados de la época actual\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "    # Guardado del mejor modelo (con menor val_loss)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(resnet18.state_dict(), MODEL_PATH)  # Guardamos el modelo\n",
    "        print(\"Mejor modelo guardado.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= EARLYSTOP_PATIENCE:  # Si no mejora durante N epochs, paramos\n",
    "            print(\"Early stopping activado.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Paso 8: Evaluación del modelo final**\n",
    "\n",
    "En esta sección vamos a:\n",
    "- Cargar el mejor modelo guardado durante el entrenamiento.\n",
    "- Evaluarlo sobre el conjunto de validación.\n",
    "- Generar el reporte de clasificación (precision, recall, F1-score, etc).\n",
    "- Visualizar la matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Cargar el mejor modelo guardado durante el entrenamiento\n",
    "resnet18.load_state_dict(torch.load(MODEL_PATH))\n",
    "resnet18.eval()  # Importante: modo evaluación desactiva dropout, batchnorm dinámica, etc.\n",
    "\n",
    "# Evaluamos sobre el conjunto de validación\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # No necesitamos gradientes en esta fase\n",
    "    for images, labels in valid_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        outputs = resnet18(images)\n",
    "        _, preds = torch.max(outputs, 1)  # Elegimos la clase con mayor probabilidad\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Reporte de métricas\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"negative\", \"positive\"]))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"negative\", \"positive\"])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Paso 9: Guardado del modelo final**\n",
    "\n",
    "Una vez que hemos evaluado el modelo y verificado su rendimiento, es hora de guardar sus pesos entrenados para su reutilización en producción o inferencia posterior.\n",
    "\n",
    "El archivo se almacenará en la carpeta de modelos del proyecto. (\"/src/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Ruta final donde se guardará el modelo entrenado\n",
    "final_model_path = \"/Users/alvarosanchez/ONLINE_DS_THEBRIDGE_ALVAROSMMS-1/ML_Clasificacion_Radiografias_Muscoesqueleticas/src/models/resnet18_final_model.joblib\"\n",
    "\n",
    "# Guardamos únicamente los pesos del modelo (state_dict)\n",
    "# Joblib guarda objetos binarios, útil para modelos en producción\n",
    "joblib.dump(resnet18.state_dict(), final_model_path)\n",
    "\n",
    "print(f\"Modelo guardado correctamente en: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
